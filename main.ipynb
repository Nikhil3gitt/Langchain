{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangChain is a powerful library designed to simplify the creation of applications powered by Large Language Models (LLMs). It enables developers to connect language models (like GPT) with external data sources (APIs, databases, or knowledge bases), perform chains of tasks, and integrate different tools to create end-to-end workflows.\n",
    "\n",
    "\n",
    "**[Langchain Crash Course](https://www.youtube.com/watch?v=yF9kGESAi3M)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Components of LangChain\n",
    "- 1. LLMs\n",
    "- 2. Prompts\n",
    "- 3. Chains\n",
    "- 4. RAGs\n",
    "- 5. Agents\n",
    "- 6. Tools \n",
    "- 7. Memory \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "*Let's setup a development environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install python-dotenv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat models (LLMs)\n",
    "\n",
    "A chat model in LangChain is a Component designed to communicate in a sytructured way with LLMs like GPT-4, Huggingface,and Etc.\n",
    "https://python.langchain.com/docs/integrations/chat/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i.e. _Hugging Face offers thousands of pretrained models available through their Model Hub. These models can be fine-tuned for specific tasks, saving time and computational resources._\n",
    "_It is widely recognized for its open-source machine learning models, tools, and datasets, which are designed to make it easier for researchers and developers to build, train, and deploy AI models._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU langchain-openai\n",
    "!pip install --upgrade langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.invoke(\"Hello, whats 9 * 2  +2\")\n",
    "\n",
    "#print(result)   # it preints whole response with metadata and other information \n",
    "\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of messages in LangChain\n",
    "\n",
    "- 1. *SystemMessage* : It's setup a Context for the conversation.\n",
    "- 2. *HumanMessage*  : Prompts that user sents/writes.\n",
    "- 3. *AiMessage*     : Ai responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "chat = ChatOpenAI(model_name=\"gpt-4\", temperature=0.7)  ## temparature higher means creativity of the response would be less \n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant specializing in providing coding advice.\"),\n",
    "    HumanMessage(content=\"Can you help me write a Python function to calculate factorial?\"),\n",
    "    AIMessage(content=\"Of course! Here's an example of a function to calculate the factorial of a number:\\n\"\n",
    "                      \"```python\\n\"\n",
    "                      \"def factorial(n):\\n\"\n",
    "                      \"    if n == 0 or n == 1:\\n\"\n",
    "                      \"        return 1\\n\"\n",
    "                      \"    else:\\n\"\n",
    "                      \"        return n * factorial(n - 1)\\n\"\n",
    "                      \"```\")\n",
    "]\n",
    "\n",
    "messages.append(HumanMessage(content=\"Can you help me prime number code?\"))\n",
    "\n",
    "response = chat(messages)\n",
    "\n",
    "messages.append(AIMessage(content=response.content))\n",
    "\n",
    "for msg in messages:\n",
    "    role = \"System\" if isinstance(msg, SystemMessage) else \"Human\" if isinstance(msg, HumanMessage) else \"AI\"\n",
    "    print(f\"{role}: {msg.content}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain transformers huggingface_hub #Hugging face\n",
    "\n",
    "!pip install -qU langchain-google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gemini-1.5-pro\n",
    "'''\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-pro\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "'''\n",
    "\n",
    "# hugging face\n",
    "\n",
    "'''\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    task=\"text-generation\",\n",
    "    max_new_tokens=512,\n",
    "    do_sample=False,\n",
    "    repetition_penalty=1.03,\n",
    ")\n",
    "\n",
    "chat_model = ChatHuggingFace(llm=llm)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = ChatOpenAI(model= \"gpt-4o\", max_tokens=100)\n",
    "\n",
    "\n",
    "human_message_template = \"Write a {genre} story about a  {character} who is in {location}.\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"human\", human_message_template)])\n",
    "\n",
    "genre = \"comedy\"\n",
    "character = \"joker\"\n",
    "location = \"city\"\n",
    "\n",
    "formatted_prompt = chat_prompt.format_messages(genre=genre, character=character, location=location)\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets make usecase which is generating Language specific story based on prompt enginnering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", max_tokens=100)\n",
    "\n",
    "# Define the prompt with placeholders for system and human messages\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a creative storyteller in {Language} Language.\"),\n",
    "    (\"human\", \"Write a {genre} story about a {character}.\")\n",
    "])\n",
    "\n",
    "formatted_prompt = chat_prompt.format_messages(Language=\"Gujarati\", genre=\"mystery\", character=\"detective\")\n",
    "\n",
    "response = llm.invoke(formatted_prompt)\n",
    "\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains \n",
    "\n",
    "*Type of chains*\n",
    "- Sequencial Chain  : linear sequences of taks one after another \n",
    "- parallel chain    : doing multiple task at same time \n",
    "- Conditional chain : next set of task triggered based on condition (i.e. If user select FAQ so app directed him towards FaQ chainif he selects Billingg then related task chain triggerd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", max_tokens=17)\n",
    "\n",
    "prompt_template = \"Tell me a about {name} \"\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"name\"], template=prompt_template\n",
    ")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "chain.invoke(\"mahuva\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Input text\n",
    "input_text = \"In today's world, artificial intelligence is revolutionizing industries like healthcare, finance, and education.\"\n",
    "\n",
    "# Initialize the model\n",
    "model = ChatOpenAI(model=\"gpt-4o\", max_tokens=120)\n",
    "\n",
    "# Define chains\n",
    "preprocess_chain = LLMChain(\n",
    "    llm=model, \n",
    "    prompt=ChatPromptTemplate.from_messages([\n",
    "        (\"user\", \"Clean the following text by removing any special characters and extra spaces: {text}\")\n",
    "    ])\n",
    ")\n",
    "summarize_chain = LLMChain(\n",
    "    llm=model, \n",
    "    prompt=ChatPromptTemplate.from_messages([\n",
    "        (\"user\", \"Summarize the following text in one paragraph: {text}\")\n",
    "    ])\n",
    ")\n",
    "translate_chain = LLMChain(\n",
    "    llm=model, \n",
    "    prompt=ChatPromptTemplate.from_messages([\n",
    "        (\"user\", \"Translate the following text into French: {text}\")\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Function to run a chain\n",
    "def run_chain(chain, text):\n",
    "    return chain.invoke({\"text\": text})[\"text\"]\n",
    "\n",
    "# Run all chains in parallel\n",
    "def run_all_chains(input_text):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(executor.map(\n",
    "            lambda task: run_chain(task[0], task[1]),\n",
    "            [(preprocess_chain, input_text), \n",
    "             (summarize_chain, input_text), \n",
    "             (translate_chain, input_text)]\n",
    "        ))\n",
    "    return {\"preprocessed\": results[0], \"summarized\": results[1], \"translated\": results[2]}\n",
    "\n",
    "# Execute and display results\n",
    "results = run_all_chains(input_text)\n",
    "print(\"Preprocessed Text:\", results[\"preprocessed\"])\n",
    "print(\"Summarized Text:\", results[\"summarized\"])\n",
    "print(\"Translated Text:\", results[\"translated\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\", max_tokens=10)\n",
    "\n",
    "sentiment_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Analyze the sentiment of the following feedback as Positive, Negative, or Neutral: {text}\")\n",
    "])\n",
    "sentiment_chain = LLMChain(llm=model, prompt=sentiment_prompt)\n",
    "\n",
    "# Positive Feedback Response Chain\n",
    "positive_feedback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Thank the user for their positive feedback and assure them of continued excellence: {text}\")\n",
    "])\n",
    "positive_feedback_chain = LLMChain(llm=model, prompt=positive_feedback_prompt)\n",
    "\n",
    "# Negative Feedback Response Chain\n",
    "negative_feedback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Apologize for the user's experience and ask for more details to resolve the issue: {text}\")\n",
    "])\n",
    "negative_feedback_chain = LLMChain(llm=model, prompt=negative_feedback_prompt)\n",
    "\n",
    "# Neutral Feedback Response Chain\n",
    "neutral_feedback_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"user\", \"Thank the user for their feedback and encourage them to share more insights in the future: {text}\")\n",
    "])\n",
    "neutral_feedback_chain = LLMChain(llm=model, prompt=neutral_feedback_prompt)\n",
    "\n",
    "# Function to run conditional chaining\n",
    "def respond_to_feedback(feedback_text):\n",
    "    # Step 1: Analyze Sentiment\n",
    "    sentiment_result = sentiment_chain.invoke({\"text\": feedback_text})[\"text\"].strip().lower()\n",
    "    \n",
    "    if \"positive\" in sentiment_result:\n",
    "        response = positive_feedback_chain.invoke({\"text\": feedback_text})[\"text\"]\n",
    "    elif \"negative\" in sentiment_result:\n",
    "        response = negative_feedback_chain.invoke({\"text\": feedback_text})[\"text\"]\n",
    "    else:\n",
    "        response = neutral_feedback_chain.invoke({\"text\": feedback_text})[\"text\"]\n",
    "    \n",
    "    return response\n",
    "\n",
    "feedback = \"The service was amazing, but I wish the delivery was faster.\"\n",
    "\n",
    "response = respond_to_feedback(feedback)\n",
    "print(\"Feedback Response:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAGs (retrieval-augmented generation)\n",
    "\n",
    "- It just give LLMs additional knowledge\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "- **Challenge with Full-Text Input**:\n",
    "  - Passing entire documents to LLMs is costly and slow due to high token usage.\n",
    "\n",
    "- **Chunking the Data**:\n",
    "  - Split text into smaller, meaningful chunks.\n",
    "  - Ensure each chunk retains context and fits within token limits.\n",
    "\n",
    "- **Using a Vector Database**:\n",
    "  - Convert chunks into embeddings using models like Sentence Transformers.\n",
    "  - Store embeddings in a vector database for efficient similarity search.\n",
    "\n",
    "- **Query-Based Retrieval**:\n",
    "  - When a question is asked, retrieve relevant chunks from the vector database.\n",
    "  - Pass only the retrieved chunks to the LLM.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![RAGs Process](./Process.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Embedding in Language Models\n",
    "\n",
    "**Contextual embeddings** refer to the way words (or tokens) are represented in a high-dimensional vector space, where their meanings are influenced by the surrounding context in a given sentence or passage. Unlike traditional static word embeddings, which assign the same vector to a word regardless of its usage, **contextual embeddings** adapt to the specific context in which a word appears.\n",
    "\n",
    "For example, consider the word **\"Apple\"** in the following two sentences:\n",
    "\n",
    "1. **\"What is Apple's revenue?\"**\n",
    "2. **\"What is Apple's calorie?\"**\n",
    "\n",
    "In the first sentence, **\"Apple\"** refers to the **tech company** (Apple Inc.), and its embedding will reflect concepts like business, technology, and finance. In the second sentence, **\"Apple\"** refers to the **fruit**, and its embedding will be shaped by concepts like food, nutrition, and health.\n",
    "\n",
    "This **context-dependent nature** of embeddings is a key feature of transformer-based models like **GPT-4**. The model generates different vector representations for the same word depending on its surrounding words, allowing it to understand and capture multiple meanings of the same token.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector embeddings\n",
    "=================\n",
    "\n",
    "Learn how to turn text into numbers, unlocking use cases like search.\n",
    "\n",
    "**New embedding models**\n",
    "\n",
    "`text-embedding-3-small` and `text-embedding-3-large`, our newest and most performant embedding models are now available, with lower costs, higher multilingual performance, and new parameters to control the overall size.\n",
    "\n",
    "What are embeddings?\n",
    "--------------------\n",
    "\n",
    "OpenAI’s text embeddings measure the relatedness of text strings. Embeddings are commonly used for:\n",
    "\n",
    "*   **Search** (where results are ranked by relevance to a query string)\n",
    "*   **Clustering** (where text strings are grouped by similarity)\n",
    "*   **Recommendations** (where items with related text strings are recommended)\n",
    "*   **Anomaly detection** (where outliers with little relatedness are identified)\n",
    "*   **Diversity measurement** (where similarity distributions are analyzed)\n",
    "*   **Classification** (where text strings are classified by their most similar label)\n",
    "\n",
    "An embedding is a vector (list) of floating point numbers. The [distance](#which-distance-function-should-i-use) between two vectors measures their relatedness. Small distances suggest high relatedness and large distances suggest low relatedness.\n",
    "\n",
    "Visit our [pricing page](https://openai.com/api/pricing/) to learn about Embeddings pricing. Requests are billed based on the number of [tokens](/tokenizer) in the [input](/docs/api-reference/embeddings/create#embeddings/create-input).\n",
    "\n",
    "How to get embeddings\n",
    "---------------------\n",
    "\n",
    "To get an embedding, send your text string to the [embeddings API endpoint](/docs/api-reference/embeddings) along with the embedding model name (e.g. `text-embedding-3-small`). The response will contain an embedding (list of floating point numbers), which you can extract, save in a vector database, and use for many different use cases:\n",
    "\n",
    "Example: Getting embeddings\n",
    "\n",
    "```javascript\n",
    "import OpenAI from \"openai\";\n",
    "const openai = new OpenAI();\n",
    "\n",
    "const embedding = await openai.embeddings.create({\n",
    "  model: \"text-embedding-3-small\",\n",
    "  input: \"Your text string goes here\",\n",
    "  encoding_format: \"float\",\n",
    "});\n",
    "\n",
    "console.log(embedding);\n",
    "```\n",
    "\n",
    "```python\n",
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "response = client.embeddings.create(\n",
    "    input=\"Your text string goes here\",\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "print(response.data[0].embedding)\n",
    "```\n",
    "\n",
    "```bash\n",
    "curl https://api.openai.com/v1/embeddings \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Bearer $OPENAI_API_KEY\" \\\n",
    "  -d '{\n",
    "    \"input\": \"Your text string goes here\",\n",
    "    \"model\": \"text-embedding-3-small\"\n",
    "  }'\n",
    "```\n",
    "\n",
    "The response will contain the embedding vector along with some additional metadata.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"object\": \"list\",\n",
    "  \"data\": [\n",
    "    {\n",
    "      \"object\": \"embedding\",\n",
    "      \"index\": 0,\n",
    "      \"embedding\": [\n",
    "        -0.006929283495992422,\n",
    "        -0.005336422007530928,\n",
    "        -4.547132266452536e-05,\n",
    "        -0.024047505110502243\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  \"model\": \"text-embedding-3-small\",\n",
    "  \"usage\": {\n",
    "    \"prompt_tokens\": 5,\n",
    "    \"total_tokens\": 5\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "By default, the length of the embedding vector will be 1536 for `text-embedding-3-small` or 3072 for `text-embedding-3-large`. You can reduce the dimensions of the embedding by passing in the [dimensions parameter](/docs/api-reference/embeddings/create#embeddings-create-dimensions) without the embedding losing its concept-representing properties. We go into more detail on embedding dimensions in the [embedding use case section](#use-cases).\n",
    "\n",
    "Embedding models\n",
    "----------------\n",
    "\n",
    "OpenAI offers two powerful third-generation embedding model (denoted by `-3` in the model ID). You can read the embedding v3 [announcement blog post](https://openai.com/blog/new-embedding-models-and-api-updates) for more details.\n",
    "\n",
    "Usage is priced per input token, below is an example of pricing pages of text per US dollar (assuming ~800 tokens per page):\n",
    "\n",
    "|Model|~ Pages per dollar|Performance on MTEB eval|Max input|\n",
    "|---|---|---|---|\n",
    "|text-embedding-3-small|62,500|62.3%|8191|\n",
    "|text-embedding-3-large|9,615|64.6%|8191|\n",
    "|text-embedding-ada-002|12,500|61.0%|8191|\n",
    "\n",
    "Use cases\n",
    "---------\n",
    "\n",
    "Here we show some representative use cases. We will use the [Amazon fine-food reviews dataset](https://www.kaggle.com/snap/amazon-fine-food-reviews) for the following examples.\n",
    "\n",
    "### Obtaining the embeddings\n",
    "\n",
    "The dataset contains a total of 568,454 food reviews Amazon users left up to October 2012. We will use a subset of 1,000 most recent reviews for illustration purposes. The reviews are in English and tend to be positive or negative. Each review has a ProductId, UserId, Score, review title (Summary) and review body (Text). For example:\n",
    "\n",
    "|Product Id|User Id|Score|Summary|Text|\n",
    "|---|---|---|---|---|\n",
    "|B001E4KFG0|A3SGXH7AUHU8GW|5|Good Quality Dog Food|I have bought several of the Vitality canned...|\n",
    "|B00813GRG4|A1D87F6ZCVE5NK|1|Not as Advertised|Product arrived labeled as Jumbo Salted Peanut...|\n",
    "\n",
    "We will combine the review summary and review text into a single combined text. The model will encode this combined text and output a single vector embedding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Watch the video here on Embedding and Vectordatabase](https://youtu.be/ySus5ZS0b94?si=Heq5WFdpx2QdaqXI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RAG example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##  Agents\n",
    "\n",
    "An **agent** in LangChain is an intelligent system that can make decisions about what actions to take based on a user’s input, using **tools** and potentially interacting with **external systems** to fulfill a task. Agents essentially act as **task managers** that coordinate the workflow between various components (like tools) and the model.\n",
    "\n",
    "### Key Roles of an Agent:\n",
    "- **Decision Making**: An agent interprets the user’s query and decides what actions are necessary to fulfill the request. This includes selecting the appropriate tool, querying a database, making an API call, or triggering some internal process.\n",
    "  \n",
    "- **Tool Management**: Agents can interact with multiple tools. They can invoke tools in a sequence or select one based on the specific task that needs to be performed. For example, if a user asks for weather information, an agent might call a weather API tool.\n",
    "\n",
    "- **Task Sequencing**: Agents can manage complex workflows that require multiple steps. For example, booking a flight might involve checking availability, calculating the price, and then booking the flight—all of which the agent can handle in sequence.\n",
    "\n",
    "### Example Agent Workflow:\n",
    "1. **User Input**: \"Book me a flight to Paris next Monday.\"\n",
    "2. **Agent**:\n",
    "   - **Interprets Intent**: Understands that the user is asking to book a flight.\n",
    "   - **Selects Tools**: Chooses relevant tools for:\n",
    "     - **Flight search** (API to check flight availability)\n",
    "     - **Price calculation** (API to check prices)\n",
    "     - **Booking** (system to complete the flight booking)\n",
    "3. **Response**: The agent completes the task, potentially providing the user with a confirmation or next steps.\n",
    "\n",
    "##  Tools\n",
    "\n",
    "A **tool** in LangChain refers to an external resource, service, or function that an agent can call to perform specific tasks. Tools provide specialized capabilities that are not natively part of the LLM’s training, like accessing databases, querying APIs, interacting with file systems, or even running complex algorithms.\n",
    "\n",
    "### Key Types of Tools:\n",
    "- **APIs**: Tools that make external API calls to retrieve or send information. For example, an API that provides weather data or currency conversion.\n",
    "  \n",
    "- **Databases**: Tools that interact with databases to retrieve or store information. For example, a tool that queries a database for a user’s order history or fetches product details.\n",
    "\n",
    "- **Calculations**: Tools that perform specific calculations or algorithms, such as running complex statistical models or doing price estimation.\n",
    "\n",
    "- **Custom Functions**: Tools can also be custom-built functions that perform tasks such as sending emails, processing data, or accessing local files.\n",
    "\n",
    "### Example Tools:\n",
    "1. **Weather API Tool**: A tool that can fetch the weather for a specific city.\n",
    "2. **Search Engine Tool**: A tool that can search the web for relevant articles or news.\n",
    "3. **Database Query Tool**: A tool that queries a database to fetch information about users, products, etc.\n",
    "\n",
    "### How Tools Are Used:\n",
    "- When a user makes a request, the agent interprets the request and identifies the necessary tools.\n",
    "- The agent calls the appropriate tool(s), receives the response, and integrates that response into the final output for the user.\n",
    "\n",
    "## Example of Agents and Tools Working Together:\n",
    "\n",
    "### Example 1: Booking a Flight\n",
    "- **User Request**: \"Book a flight from New York to London next week.\"\n",
    "- **Agent**: \n",
    "  - The agent decides that it needs to check flight availability (tool: **Flight API**).\n",
    "  - The agent queries the **Flight API** to get available flights.\n",
    "  - The agent uses another tool to calculate the **price** for the flight (tool: **Price Calculation API**).\n",
    "  - The agent then books the flight using the booking tool (tool: **Flight Booking API**).\n",
    "  \n",
    "  The agent combines the responses from each tool to present the user with a confirmed flight.\n",
    "\n",
    "### Example 2: Weather Forecast\n",
    "- **User Request**: \"What’s the weather like in Tokyo tomorrow?\"\n",
    "- **Agent**: \n",
    "  - The agent calls the **Weather API Tool** to get tomorrow's weather in Tokyo.\n",
    "  - The agent processes the response (e.g., \"sunny with a high of 25°C\") and presents it to the user.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agent and tool example "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory \n",
    "\n",
    "**Memory** in LangChain refers to the ability of a language model (LLM) to retain and recall previous interactions, data, or context over multiple steps in a conversation or task. This is crucial in many real-world applications where the model needs to maintain consistency, track conversation history, or store intermediate results for future reference.\n",
    "\n",
    "LangChain provides different types of memory mechanisms that can help LLMs remember previous interactions or relevant data. This allows the agent to perform more dynamic, context-aware tasks without having to repeat the entire process or re-examine all previous data.\n",
    "\n",
    "## Key Concepts of Memory in LangChain\n",
    "\n",
    "### 1. Persistent Memory\n",
    "   - This type of memory stores information across multiple interactions, even if the LLM or agent is restarted. This is useful for applications like chatbots, virtual assistants, or systems where context needs to be maintained over time.\n",
    "   - **Example**: A chatbot that remembers a user’s preferences or a customer support agent that tracks open tickets.\n",
    "\n",
    "### 2. Short-term Memory\n",
    "   - Short-term memory is typically used to hold the context within a single session or task. It helps the agent remember relevant information as long as the task is ongoing but doesn't persist once the session ends.\n",
    "   - **Example**: In a task like booking a flight, the agent might remember the departure city, destination, and travel dates throughout the interaction but forget them after the task is completed.\n",
    "\n",
    "### 3. Memory Types in LangChain:\n",
    "   - **ConversationBufferMemory**: A simple memory that stores a buffer of conversation history, used for handling conversational context.\n",
    "   - **VectorStoreMemory**: Stores information as vectors (embeddings), which is useful when dealing with large datasets or when performing tasks like semantic search.\n",
    "   - **BufferMemory**: Stores a series of interactions or events in a queue-like fashion, useful for tracking a sequence of actions or operations.\n",
    "\n",
    "### 4. Memory in Agents and Tools:\n",
    "   - Agents can use memory to store the state of an ongoing task. For example, if an agent is processing a multi-step workflow (like booking a flight), memory can store intermediate results such as available flights, user preferences, or completed steps.\n",
    "   - Tools can be designed to interact with memory to retrieve or update information during an agent's decision-making process.\n",
    "\n",
    "## Benefits of Memory in LangChain\n",
    "- **Contextual Understanding**: Memory allows an agent to maintain continuity in conversations or tasks, which improves the user experience.\n",
    "- **Efficiency**: By remembering past information, the agent can reduce redundant queries or actions, leading to more efficient processing.\n",
    "- **Flexibility**: Memory enables more complex and dynamic workflows by allowing the agent to adapt to the situation based on previously stored information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
